{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Boxouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mtick\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests   \n",
    "import shutil      \n",
    "import datetime\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import winsound\n",
    "\n",
    "home_folder = 'C:\\\\Users\\\\Travis\\\\OneDrive\\\\Data Science\\\\Personal_Projects\\\\Sports\\\\NBA_Prediction_V3_1'\n",
    "os.chdir(home_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_name_values(filename):\n",
    "        # replace values with dashes for compatibility\n",
    "    filename = filename.replace('%','_')\n",
    "    filename = filename.replace('=','_')\n",
    "    filename = filename.replace('?','_')\n",
    "    filename = filename.replace('&','_')\n",
    "    filename = filename.replace('20Season_','')\n",
    "    filename = filename.replace('20Season','')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_player_data(url_list, file_folder):    \n",
    "        \n",
    "        # Scrape Season-Level player data from the url_list\n",
    "\n",
    "        i = 0\n",
    "        for u in url_list:\n",
    "                \n",
    "                driver.get(u)\n",
    "                time.sleep(2)\n",
    "\n",
    "                # if the page does not load, go to the next in the list\n",
    "                try:\n",
    "                        xpath = '//*[@id=\"__next\"]/div[2]/div[2]/div[3]/section[2]/div/div[2]/div[2]/div[1]/div[3]/div/label/div/select/option[1]'\n",
    "                        elem = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "                except:\n",
    "                        print(f'{u} did not load. Moving to next url.')\n",
    "                        continue\n",
    "\n",
    "                # click \"all pages\"\n",
    "                xpath_all = '//*[@id=\"__next\"]/div[2]/div[2]/div[3]/section[2]/div/div[2]/div[2]/div[1]/div[3]/div/label/div/select/option[1]' \n",
    "                elem = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, xpath_all)))\n",
    "                \n",
    "                driver.find_element(by=By.XPATH, value=xpath_all).click()\n",
    "                src = driver.page_source\n",
    "                parser = BeautifulSoup(src, \"lxml\")\n",
    "                table = parser.find(\"table\", attrs = {\"class\":\"Crom_table__p1iZz\"})\n",
    "                headers = table.findAll('th')\n",
    "                headerlist = [h.text.strip() for h in headers[0:]] \n",
    "                row_names = table.findAll('a')                             # find rows\n",
    "                row_list = [b.text.strip() for b in row_names[0:]] \n",
    "                rows = table.findAll('tr')[0:]\n",
    "                player_stats = [[td.getText().strip() for td in rows[i].findAll('td')[0:]] for i in range(len(rows))]\n",
    "                tot_cols = len(player_stats[1])                           #set the length to ignore hidden columns\n",
    "                headerlist = headerlist[:tot_cols]   \n",
    "                stats = pd.DataFrame(player_stats, columns = headerlist)\n",
    "\n",
    "                # assign filename\n",
    "                filename = file_folder + str(u[34:]).replace('/', '_') + '.csv'\n",
    "                filename = replace_name_values(filename)\n",
    "                pd.DataFrame.to_csv(stats, filename)\n",
    "                i += 1\n",
    "                lu = len(url_list)\n",
    "                # close driver\n",
    "                print(f'{filename} Completed Successfully! {i} / {lu} Complete!')\n",
    "\n",
    "        winsound.Beep(523, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_the_data(folder, data_prefix, filename_selector):\n",
    "    # Appending data together via folder and/or file name\n",
    "\n",
    "    path = folder\n",
    "    p = os.listdir(path)\n",
    "    pf = pd.DataFrame(p)\n",
    "\n",
    "\n",
    "    # filter for files that contain the filename_selector\n",
    "    pf_reg = pf.loc[pf[0].astype(str).str.contains(filename_selector)] \n",
    "\n",
    "    appended_data = []\n",
    "    for file in pf_reg[0]:\n",
    "        data = pd.read_csv(folder + '/' + file)\n",
    "        # if \"Season\" a column, drop it\n",
    "        if 'Season' in data.columns:\n",
    "            data = data.drop(columns = ['Season'])\n",
    "        \n",
    "        data['season'] = file[(file.find('20')):(file.find('20'))+4]\n",
    "        data['season_type'] = np.where('Regular' in file, 'Regular', 'Playoffs')\n",
    "        # add prefix to columns\n",
    "        data = data.add_prefix(data_prefix)\n",
    "        data.columns = data.columns.str.lower()\n",
    "        appended_data.append(data)\n",
    "    \n",
    "    appended_data = pd.concat(appended_data)\n",
    "    return appended_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_boxouts = 'https://www.nba.com/stats/players/box-outs/'\n",
    "boxouts_urls = []\n",
    "years =['2021-22', '2020-21', '2019-20', '2018-19', '2017-18']\n",
    "season_types = ['Regular%20Season', 'Playoffs']\n",
    "\n",
    "for year in years:\n",
    "    for s_types in season_types:\n",
    "        url = player_boxouts + '?Season=' + year + '&SeasonType=' + s_types\n",
    "        boxouts_urls.append(str(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the files to the correct folder\n",
    "for file in os.listdir('data/player/boxouts/'):\n",
    "    if '.csv' in file:\n",
    "        if 'Playoffs' in file:\n",
    "            os.rename('data/player/boxouts/' + file, 'data/player/boxouts/playoffs/' + file)\n",
    "        else:\n",
    "            os.rename('data/player/boxouts/' + file, 'data/player/boxouts/regular_season/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxouts = append_the_data('data/player/boxouts/regular_season', 'boxouts_', 'box-outs')\n",
    "boxouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxouts.to_csv('data/player/aggregates/All_Boxouts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trav310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f69e36f0e9b2c8d9f319b417484f14b77c91d7bef950ad448542405eb1e0e594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
